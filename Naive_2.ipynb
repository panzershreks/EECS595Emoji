{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op7G6AxCHgBY"
      },
      "outputs": [],
      "source": [
        "# Auto sync for editing python script file in real time\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "TLlTnbV3Hip_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"EECS595/Emoji\""
      ],
      "metadata": {
        "id": "XhX2tmYjHkrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/EECS595/Emoji"
      ],
      "metadata": {
        "id": "AJHQhohHHn9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "assert sys.version_info[0] == 3\n",
        "assert sys.version_info[1] >= 5\n",
        "\n",
        "GOOGLE_DRIVE_PATH = os.path.join(\"/content\", \"drive\", \"My Drive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "sys.path.append(GOOGLE_DRIVE_PATH)\n",
        "\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))\n",
        "\n",
        "assert \"Embedding.ipynb\" in os.listdir(GOOGLE_DRIVE_PATH), \"<Warning>: Embedding.ipynb not found.\"\n",
        "# assert \"Embedding.py\" in os.listdir(\n",
        "#     GOOGLE_DRIVE_PATH\n",
        "# ), \"<Warning>: Embedding not found.\""
      ],
      "metadata": {
        "id": "pmY_wbdvHp_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "jHeSPU28Hslo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install emoji"
      ],
      "metadata": {
        "id": "PeOZwYhiHtkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizerFast, BertModel, Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch import nn\n",
        "import emoji as emo\n",
        "from tqdm import tqdm\n",
        "import concurrent.futures\n",
        "import multiprocessing as mp\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import unicodedata as ud\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from itertools import repeat\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
      ],
      "metadata": {
        "id": "JAQ-EeZQHwtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naïve sentiment analysis of a tweet\n",
        "\n",
        "basic algorithm: \n",
        "1. if all blatantly positive emoji - positive\n",
        "2. if all blatantly negative emoji - negative \n",
        "3. random assign"
      ],
      "metadata": {
        "id": "4bOQ3rlJHy4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naïve Classification Pipeline\n",
        "1. Load rehydrated twitter data \n",
        "2. Split into train and test sets\n",
        "3. v1. Make emoji dictionary from TM Senti; v2. Incorporate EmoTag 620 into the dict\n",
        "4. Perform sentiment analysis with the alg"
      ],
      "metadata": {
        "id": "nVEvHDyDH_SW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Load Data"
      ],
      "metadata": {
        "id": "i3KwCTUUIB12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emo_pos = ['😀','😃','😄','😁','😆','😊','😇','😌','😍','🥰','🤩','😘','😗','😚','😙','😋','😝','🤗','🤤','🥳','😎','😺','😸','😻','😽','✌️','👍','❤️','💕','♥️','💙','💜','💗','🖤','💛','💖','💞','💚','❣️','💓','🧡','💘','💝','💟','🤍','🤎']\n",
        "emo_beg = ['🙃','😐','😑','😶','😒','🙄','😬','😔','🤢','🤮','😵‍💫','😕','🙁','☹️','😟','😧','😨','😰','😢','😭','👎','😖','😣','😞','😓','😩','😫','😠','😡','🤬','😿','🖕','😥','💔']\n",
        "\n",
        "emoticon_pos = [':)', ':-)', ':)', ':D', '=D', ':-]', ':]', ':-3', ':3', ':->', ':>', '8-)', ':-}', ':}', ':o)', ':c)', ':', 'ˆ)', '=]', '=)', ':D', '8D', 'xD', '=D', '=3', 'BDˆ', ':-))', ':’)', ';)', ';)', '*-)', '*)', ';]', ';ˆ)', ':,', ';D', '<3']\n",
        "emoticon_neg = [':(',':-(',':(', ':’(', ':(', ':c',':<',':<',':[', ':[', ':-||', '>:[', ':{ ',':@', '>:(', 'D’:', 'D:<', 'D:', 'D;', 'D=', ':/', ':/', ':.', '>:/' , '>:/', ':\\', '=/', '=\\', ':L', '=L', ':S', '</3', '<\\']"
      ],
      "metadata": {
        "id": "Wvpmqln6QZl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parquet_dir = './rehydrated_tweets_parquet'"
      ],
      "metadata": {
        "id": "wEmV7uiEINq4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet('./rehydrated_tweets_parquet/2013/01/en_2013_01_29.parquet')\n",
        "# pd.read_parquet('en_2013_01_29.parquet', engine='pyarrow')\n",
        "df"
      ],
      "metadata": {
        "id": "rySdMxZBIOMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. v1. Make emoji list from TM Senti\n",
        "Pos2013-2020: 😍🥰😌👍😊😘:) ;) :D\n",
        "Neg2013-2020: 😭 :( 😔🙄😢😩 :/ 😒"
      ],
      "metadata": {
        "id": "AWB9CPZvIZYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotm_pos = ['😍', '🥰', '😌', '👍', '😊', '😘', ':)', ';)', ':D']\n",
        "emotm_neg =['😭', '😔', '🙄', '😢', '😩', '😒', ':/', ':(' ]\n",
        "print(emotm_pos)\n",
        "print(emotm_neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPcVCCDBIefz",
        "outputId": "ce1a21b2-d2c0-4000-cc0a-d80dc03eb483"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['😍', '🥰', '😌', '👍', '😊', '😘', ':)', ';)', ':D']\n",
            "['😭', '😔', '🙄', '😢', '😩', '😒', ':/', ':(']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a set of emojis from df `tweet_text` col"
      ],
      "metadata": {
        "id": "DMrpE8BuId1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emoset = set()\n",
        "v = df['tweet_text'].values\n",
        "for tweet_str in tqdm(v):\n",
        "  for char in tweet_str:\n",
        "    if emo.is_emoji(char):\n",
        "      emoset.add(char)\n",
        "emolst = list(emoset)\n",
        "print(emoset)\n",
        "print(emolst)"
      ],
      "metadata": {
        "id": "fE18j6yhIk7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "emoticon list "
      ],
      "metadata": {
        "id": "psXApJQZIqXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emov = df_sm['emojis'].values.tolist()\n",
        "\n",
        "emoticons = set()\n",
        "for emoji_str in tqdm(emov):\n",
        "    if ',' in emoji_str:\n",
        "        emoji_list = emoji_str.split(',')\n",
        "        for emoji in emoji_list:\n",
        "          if not emo.is_emoji(emoji):\n",
        "            # if not emo.is_emoji(emoji) and emoji not in emoticons:\n",
        "            emoticons.add(emoji)\n",
        "    else:\n",
        "        if not emo.is_emoji(emoji_str):\n",
        "            emoticons.add(emoji_str)\n",
        "emoticonslst = list(emoticons)\n",
        "print(emoticonslst)"
      ],
      "metadata": {
        "id": "TSFDj0kCIrtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "somehow this function goes in infiite loops"
      ],
      "metadata": {
        "id": "JoCumMWXIzxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emov = df_sm['emojis'].values.tolist()\n",
        "\n",
        "def extract_emoticons(emov):\n",
        "  emoticons = set()\n",
        "  for emoji_str in tqdm(emov):\n",
        "    if ',' in emoji_str:\n",
        "        emoji_list = emoji_str.split(',')\n",
        "        for emoji in emoji_list:\n",
        "          if not emo.is_emoji(emoji):\n",
        "            # if not emo.is_emoji(emoji) and emoji not in emoticons:\n",
        "            emoticons.add(emoji)\n",
        "    else:\n",
        "        if not emo.is_emoji(emoji_str):\n",
        "            emoticons.add(emoji_str)\n",
        "  emoticonslst = list(emoticons)\n",
        "  return emoticonslst\n",
        "  \n",
        "df['extracted_emoticons'] = df.apply(lambda row : extract_emoticons(row['emojis']), axis = 1)\n",
        "df['extracted_emoticons'].head(5)"
      ],
      "metadata": {
        "id": "Vtjb9skOIz54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "emojis tweet by tweet"
      ],
      "metadata": {
        "id": "XVPc59_qI-gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lst = df['tweet_text'].values.tolist()\n",
        "lstemo = df['emojis'].values.tolist()\n",
        "\n",
        "def extract_emojis(lst):\n",
        "  # print(lst)\n",
        "  extracted_emojis = []\n",
        "  # extracted_emojis2 = []\n",
        "  for tweet in lst:\n",
        "    for word in tweet:\n",
        "      if emo.is_emoji(word) and word not in extracted_emojis:\n",
        "        extracted_emojis.append(word)\n",
        "        # extracted_emojis2 = extracted_emojis + emoticons\n",
        "  return extracted_emojis\n",
        "\n",
        "df['extracted_emojis'] = df.apply(lambda row : extract_emojis(row['tweet_text']), axis = 1)\n",
        "df_sm['extracted_emojis'] = df_sm.apply(lambda row : extract_emojis(row['tweet_text']), axis = 1)\n",
        "\n",
        "df['extracted_emojis'].head(5)"
      ],
      "metadata": {
        "id": "NIwexK87JDxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "u3xvoOd1JIgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_list = df_sm['extracted_emojis'].values.tolist()\n",
        "print(extracted_list)\n",
        "print(len(extracted_list))"
      ],
      "metadata": {
        "id": "Cu0nzQUjJaVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*classification*:\n",
        "if all emojis in `extracted_emojis` is positive - positive\n",
        "if all emojis in `extracted_emojis` is negative - negative \n",
        "rest - randomly assign label\n",
        "def score(extracted_list):\n",
        "\n",
        "\n",
        "https://www.geeksforgeeks.org/python-check-if-one-list-is-subset-of-other/\n"
      ],
      "metadata": {
        "id": "y86ZFLT1JQzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pseudo Code\n",
        "for lst in extracted_list:\n",
        "  if lst is a subset of pos collection:\n",
        "    position_to_cursor\n",
        "    elif list is a subset of neg collection:\n",
        "      neg\n",
        "      else:\n",
        "        random\n",
        "#def score(extracted_list):"
      ],
      "metadata": {
        "id": "fT5CmGOzJRko"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}